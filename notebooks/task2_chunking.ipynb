{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db8b233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# # Task 2: Text Chunking, Embedding, and Vector Store Indexing\n",
    "# \n",
    "# ## üéØ Objective\n",
    "# Convert cleaned text narratives into a format suitable for efficient semantic search.\n",
    "# \n",
    "# Since the challenge provides pre-built embeddings, we'll:\n",
    "# 1. Analyze the pre-built embeddings structure\n",
    "# 2. Create a small sample for learning purposes\n",
    "# 3. Document our chunking and embedding strategy\n",
    "# \n",
    "\n",
    "# %%\n",
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set style\n",
    "plt.style.use('default')\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764e8f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Checking available data files...\n",
      "Found 4 data files:\n",
      "  ‚Ä¢ complaint_embeddings.parquet: 2289.7 MB\n",
      "  ‚Ä¢ complaints.csv: 5762.3 MB\n",
      "  ‚Ä¢ filtered_complaints.csv: 0.1 MB\n",
      "  ‚Ä¢ filtered_complaints_sample.csv: 0.1 MB\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# ## Step 1: Check Available Files\n",
    "\n",
    "# %%\n",
    "print(\"üìÅ Checking available data files...\")\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk(\"../data\"):\n",
    "    for file in files:\n",
    "        if file.endswith(('.csv', '.parquet')):\n",
    "            full_path = os.path.join(root, file)\n",
    "            size_mb = os.path.getsize(full_path) / (1024**2)\n",
    "            data_files.append((file, size_mb, full_path))\n",
    "\n",
    "print(f\"Found {len(data_files)} data files:\")\n",
    "for file, size_mb, path in sorted(data_files):\n",
    "    print(f\"  ‚Ä¢ {file}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b0c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üì• Loading pre-built embeddings metadata...\n",
      "‚ùå Error: No match for FieldRef.Name(complaint_id) in id: string\n",
      "document: string\n",
      "embedding: list<element: double>\n",
      "metadata: struct<chunk_index: int64, company: string, complaint_id: string, date_received: string, issue: string, product: string, product_category: string, state: string, sub_issue: string, total_chunks: int64>\n",
      "__fragment_index: int32\n",
      "__batch_index: int32\n",
      "__last_in_fragment: bool\n",
      "__filename: string\n",
      "Creating sample data for demonstration...\n"
     ]
    }
   ],
   "source": [
    "# ## Step 2: Load Pre-built Embeddings Metadata\n",
    "\n",
    "# %%\n",
    "print(\"\\nüì• Loading pre-built embeddings metadata...\")\n",
    "\n",
    "embeddings_path = \"../data/raw/complaint_embeddings.parquet\"\n",
    "\n",
    "if os.path.exists(embeddings_path):\n",
    "    try:\n",
    "        # Read just metadata columns (fast, no vectors)\n",
    "        metadata_cols = [\n",
    "            'complaint_id', 'product_category', 'product', 'issue',\n",
    "            'sub_issue', 'company', 'state', 'date_received',\n",
    "            'chunk_index', 'total_chunks'\n",
    "        ]\n",
    "        \n",
    "        # Use pyarrow for fast parquet reading\n",
    "        df_embeddings = pd.read_parquet(\n",
    "            embeddings_path,\n",
    "            columns=metadata_cols,\n",
    "            engine='pyarrow'\n",
    "        )\n",
    "        \n",
    "        print(f\"‚úÖ Metadata loaded successfully!\")\n",
    "        print(f\"üìä Total chunks in embeddings: {len(df_embeddings):,}\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\nüëÄ First 3 rows of metadata:\")\n",
    "        display(df_embeddings.head(3))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "        print(\"Creating sample data for demonstration...\")\n",
    "        df_embeddings = None\n",
    "else:\n",
    "    print(\"‚ùå Embeddings file not found at:\", embeddings_path)\n",
    "    df_embeddings = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ee2ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "ANALYZING EMBEDDINGS STRUCTURE\n",
      "============================================================\n",
      "‚ö†Ô∏è No embeddings data to analyze\n"
     ]
    }
   ],
   "source": [
    "# ## Step 3: Analyze Embeddings Structure\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"ANALYZING EMBEDDINGS STRUCTURE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "if df_embeddings is not None:\n",
    "    # Product distribution\n",
    "    if 'product_category' in df_embeddings.columns:\n",
    "        print(\"üìä Product Category Distribution:\")\n",
    "        product_counts = df_embeddings['product_category'].value_counts()\n",
    "        \n",
    "        # Plot\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        top_products = product_counts.head(10)\n",
    "        bars = plt.barh(range(len(top_products)), top_products.values)\n",
    "        plt.yticks(range(len(top_products)), top_products.index)\n",
    "        plt.xlabel('Number of Chunks')\n",
    "        plt.title('Top 10 Product Categories (by chunk count)')\n",
    "        plt.grid(True, alpha=0.3, axis='x')\n",
    "        \n",
    "        # Add labels\n",
    "        for i, (bar, value) in enumerate(zip(bars, top_products.values)):\n",
    "            plt.text(value + max(top_products.values)*0.01, \n",
    "                    i, \n",
    "                    f'{value:,}', \n",
    "                    va='center',\n",
    "                    fontsize=9)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print counts\n",
    "        print(\"\\nüìà Product counts:\")\n",
    "        for product, count in product_counts.head(10).items():\n",
    "            percentage = count / len(df_embeddings) * 100\n",
    "            print(f\"  ‚Ä¢ {product}: {count:,} chunks ({percentage:.1f}%)\")\n",
    "    \n",
    "    # Chunk statistics\n",
    "    if 'chunk_index' in df_embeddings.columns and 'total_chunks' in df_embeddings.columns:\n",
    "        print(f\"\\nüìù Chunking Statistics:\")\n",
    "        print(f\"  ‚Ä¢ Total complaints: {df_embeddings['complaint_id'].nunique():,}\")\n",
    "        print(f\"  ‚Ä¢ Total chunks: {len(df_embeddings):,}\")\n",
    "        print(f\"  ‚Ä¢ Average chunks per complaint: {df_embeddings['total_chunks'].mean():.2f}\")\n",
    "        print(f\"  ‚Ä¢ Max chunks per complaint: {df_embeddings['total_chunks'].max()}\")\n",
    "        print(f\"  ‚Ä¢ Min chunks per complaint: {df_embeddings['total_chunks'].min()}\")\n",
    "        \n",
    "        # Distribution of chunks per complaint\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        chunk_dist = df_embeddings['total_chunks'].value_counts().head(20)\n",
    "        chunk_dist.plot(kind='bar')\n",
    "        plt.title('Distribution of Chunks per Complaint')\n",
    "        plt.xlabel('Number of Chunks')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    # Check target products\n",
    "    target_products = ['Credit Card', 'Personal Loan', 'Savings Account', 'Money transfers']\n",
    "    print(f\"\\nüéØ Checking target products:\")\n",
    "    if 'product_category' in df_embeddings.columns:\n",
    "        for product in target_products:\n",
    "            # Case-insensitive search\n",
    "            mask = df_embeddings['product_category'].str.contains(product, case=False, na=False)\n",
    "            count = mask.sum()\n",
    "            if count > 0:\n",
    "                print(f\"  ‚Ä¢ {product}: {count:,} chunks found\")\n",
    "            else:\n",
    "                print(f\"  ‚Ä¢ {product}: Not found (checking variations...)\")\n",
    "                # Try different variations\n",
    "                variations = [product.lower(), product.upper(), product.title()]\n",
    "                for var in variations:\n",
    "                    var_mask = df_embeddings['product_category'] == var\n",
    "                    if var_mask.any():\n",
    "                        print(f\"    Found as '{df_embeddings[var_mask].iloc[0]['product_category']}'\")\n",
    "                        break\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No embeddings data to analyze\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d87162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
