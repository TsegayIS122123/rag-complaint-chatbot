{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1db8b233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Libraries imported\n",
      "üìÅ Checking files (FAST)...\n",
      "Found 4 files:\n",
      "  ‚Ä¢ complaint_embeddings.parquet: 2289.7 MB\n",
      "  ‚Ä¢ complaints.csv: 5762.3 MB\n",
      "  ‚Ä¢ filtered_complaints.csv: 0.1 MB\n",
      "  ‚Ä¢ filtered_complaints_sample.csv: 0.1 MB\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "print(\"‚úÖ Libraries imported\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 1: Quick File Check\n",
    "\n",
    "# %%\n",
    "print(\"üìÅ Checking files (FAST)...\")\n",
    "\n",
    "# Just list files without loading\n",
    "data_files = []\n",
    "for root, dirs, files in os.walk(\"../data\"):\n",
    "    for file in files:\n",
    "        if file.endswith(('.csv', '.parquet')):\n",
    "            full_path = os.path.join(root, file)\n",
    "            size_mb = os.path.getsize(full_path) / (1024**2)\n",
    "            data_files.append((file, size_mb))\n",
    "\n",
    "print(f\"Found {len(data_files)} files:\")\n",
    "for file, size_mb in sorted(data_files):\n",
    "    print(f\"  ‚Ä¢ {file}: {size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764e8f70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PRE-BUILT EMBEDDINGS SPECIFICATIONS\n",
      "============================================================\n",
      "üìã Embeddings Specifications:\n",
      "  ‚Ä¢ total_complaints: 464000\n",
      "  ‚Ä¢ total_chunks: 1370000\n",
      "  ‚Ä¢ embedding_model: sentence-transformers/all-MiniLM-L6-v2\n",
      "  ‚Ä¢ dimensions: 384\n",
      "  ‚Ä¢ vector_database: ChromaDB\n",
      "  ‚Ä¢ chunk_size: 500\n",
      "  ‚Ä¢ chunk_overlap: 50\n",
      "  ‚Ä¢ file_size_gb: 2.2\n",
      "  ‚Ä¢ metadata_fields:\n",
      "    - complaint_id\n",
      "    - product_category\n",
      "    - product\n",
      "    - issue\n",
      "    - sub_issue\n",
      "    - company\n",
      "    - state\n",
      "    - date_received\n",
      "    - chunk_index\n",
      "    - total_chunks\n"
     ]
    }
   ],
   "source": [
    "# ## Step 2: Document Pre-built Embeddings (From Challenge Specs)\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"PRE-BUILT EMBEDDINGS SPECIFICATIONS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# From challenge description\n",
    "embeddings_specs = {\n",
    "    \"total_complaints\": 464000,\n",
    "    \"total_chunks\": 1370000,\n",
    "    \"embedding_model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n",
    "    \"dimensions\": 384,\n",
    "    \"vector_database\": \"ChromaDB\",\n",
    "    \"chunk_size\": 500,\n",
    "    \"chunk_overlap\": 50,\n",
    "    \"file_size_gb\": 2.2,\n",
    "    \"metadata_fields\": [\n",
    "        \"complaint_id\",\n",
    "        \"product_category\",\n",
    "        \"product\",\n",
    "        \"issue\",\n",
    "        \"sub_issue\",\n",
    "        \"company\",\n",
    "        \"state\",\n",
    "        \"date_received\",\n",
    "        \"chunk_index\",\n",
    "        \"total_chunks\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"üìã Embeddings Specifications:\")\n",
    "for key, value in embeddings_specs.items():\n",
    "    if isinstance(value, list):\n",
    "        print(f\"  ‚Ä¢ {key}:\")\n",
    "        for item in value:\n",
    "            print(f\"    - {item}\")\n",
    "    else:\n",
    "        print(f\"  ‚Ä¢ {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "43b0c881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CREATING MINIMAL SAMPLE (10K chunks)\n",
      "============================================================\n",
      "Creating sample of 10,000 chunks...\n",
      "‚úÖ Sample created: 10,000 chunks\n",
      "\n",
      "üìä Sample Distribution:\n",
      "  ‚Ä¢ Credit card: 4,033 (40.3%)\n",
      "  ‚Ä¢ Personal loan: 3,019 (30.2%)\n",
      "  ‚Ä¢ Savings account: 1,968 (19.7%)\n",
      "  ‚Ä¢ Money transfers: 980 (9.8%)\n",
      "\n",
      "üíæ Sample saved to: ../data/processed/sample_chunks.csv\n"
     ]
    }
   ],
   "source": [
    "# ## Step 3: Create Minimal Sample for Learning\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CREATING MINIMAL SAMPLE (10K chunks)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create a small sample instantly (no loading of 2.2GB file)\n",
    "sample_size = 10000\n",
    "\n",
    "print(f\"Creating sample of {sample_size:,} chunks...\")\n",
    "\n",
    "# Create synthetic sample for demonstration\n",
    "np.random.seed(42)\n",
    "sample_data = {\n",
    "    'complaint_id': np.random.randint(1, 50000, sample_size),\n",
    "    'product_category': np.random.choice(\n",
    "        ['Credit card', 'Personal loan', 'Savings account', 'Money transfers'],\n",
    "        sample_size,\n",
    "        p=[0.4, 0.3, 0.2, 0.1]\n",
    "    ),\n",
    "    'chunk_index': np.random.randint(1, 5, sample_size),\n",
    "    'total_chunks': np.random.randint(1, 8, sample_size),\n",
    "    'text_preview': [f\"Complaint chunk about issue {i}\" for i in range(sample_size)]\n",
    "}\n",
    "\n",
    "df_sample = pd.DataFrame(sample_data)\n",
    "print(f\"‚úÖ Sample created: {len(df_sample):,} chunks\")\n",
    "\n",
    "# Show distribution\n",
    "print(\"\\nüìä Sample Distribution:\")\n",
    "product_counts = df_sample['product_category'].value_counts()\n",
    "for product, count in product_counts.items():\n",
    "    percentage = count / len(df_sample) * 100\n",
    "    print(f\"  ‚Ä¢ {product}: {count:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Save sample\n",
    "sample_path = \"../data/processed/sample_chunks.csv\"\n",
    "df_sample.to_csv(sample_path, index=False)\n",
    "print(f\"\\nüíæ Sample saved to: {sample_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d7ee2ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "CHUNKING STRATEGY DOCUMENTATION\n",
      "============================================================\n",
      "\n",
      "üî™ **Chunking Approach:**\n",
      "\n",
      "**Parameters (from challenge specs):**\n",
      "- Chunk size: 500 characters\n",
      "- Chunk overlap: 50 characters\n",
      "- Method: Recursive character splitting\n",
      "\n",
      "**Why these parameters:**\n",
      "1. **500 characters**: Captures typical complaint paragraphs\n",
      "2. **50 overlap**: Ensures context preservation across chunks\n",
      "3. **Recursive splitting**: Handles varying text lengths naturally\n",
      "\n",
      "**Example calculation:**\n",
      "A 1500-character complaint would be chunked as:\n",
      "- Chunk 1: chars 0-500\n",
      "- Chunk 2: chars 450-950 (50 char overlap)\n",
      "- Chunk 3: chars 900-1400 (50 char overlap)\n",
      "- Chunk 4: chars 1350-1500 (if needed)\n",
      "\n",
      "**Average chunks per complaint:**\n",
      "Total chunks (1.37M) / Total complaints (464K) = ~3 chunks per complaint\n",
      "\n",
      "\n",
      "============================================================\n",
      "EMBEDDING MODEL DOCUMENTATION\n",
      "============================================================\n",
      "\n",
      "ü§ñ **Model: sentence-transformers/all-MiniLM-L6-v2**\n",
      "\n",
      "**Selection Justification:**\n",
      "\n",
      "1. **Challenge Requirement**: Used in pre-built embeddings\n",
      "2. **Efficiency**: 384 dimensions (faster than 768D models)\n",
      "3. **Accuracy**: Optimized for semantic similarity tasks\n",
      "4. **Size**: ~80MB (easy to deploy)\n",
      "5. **Speed**: Fast inference for real-time retrieval\n",
      "\n",
      "**Technical Specifications:**\n",
      "- Dimensions: 384\n",
      "- Max sequence: 256 word pieces\n",
      "- Training: 1B+ sentence pairs\n",
      "- Use case: Semantic search, clustering\n",
      "\n",
      "**For Financial Complaints:**\n",
      "- Captures semantic meaning of complaint narratives\n",
      "- Works well with 500-character chunks\n",
      "- Efficient for searching 1.37M chunks\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ## Step 4: Document Chunking Strategy\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"CHUNKING STRATEGY DOCUMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "üî™ **Chunking Approach:**\n",
    "\n",
    "**Parameters (from challenge specs):**\n",
    "- Chunk size: 500 characters\n",
    "- Chunk overlap: 50 characters\n",
    "- Method: Recursive character splitting\n",
    "\n",
    "**Why these parameters:**\n",
    "1. **500 characters**: Captures typical complaint paragraphs\n",
    "2. **50 overlap**: Ensures context preservation across chunks\n",
    "3. **Recursive splitting**: Handles varying text lengths naturally\n",
    "\n",
    "**Example calculation:**\n",
    "A 1500-character complaint would be chunked as:\n",
    "- Chunk 1: chars 0-500\n",
    "- Chunk 2: chars 450-950 (50 char overlap)\n",
    "- Chunk 3: chars 900-1400 (50 char overlap)\n",
    "- Chunk 4: chars 1350-1500 (if needed)\n",
    "\n",
    "**Average chunks per complaint:**\n",
    "Total chunks (1.37M) / Total complaints (464K) = ~3 chunks per complaint\n",
    "\"\"\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 5: Document Embedding Model Choice\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EMBEDDING MODEL DOCUMENTATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "ü§ñ **Model: sentence-transformers/all-MiniLM-L6-v2**\n",
    "\n",
    "**Selection Justification:**\n",
    "\n",
    "1. **Challenge Requirement**: Used in pre-built embeddings\n",
    "2. **Efficiency**: 384 dimensions (faster than 768D models)\n",
    "3. **Accuracy**: Optimized for semantic similarity tasks\n",
    "4. **Size**: ~80MB (easy to deploy)\n",
    "5. **Speed**: Fast inference for real-time retrieval\n",
    "\n",
    "**Technical Specifications:**\n",
    "- Dimensions: 384\n",
    "- Max sequence: 256 word pieces\n",
    "- Training: 1B+ sentence pairs\n",
    "- Use case: Semantic search, clustering\n",
    "\n",
    "**For Financial Complaints:**\n",
    "- Captures semantic meaning of complaint narratives\n",
    "- Works well with 500-character chunks\n",
    "- Efficient for searching 1.37M chunks\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "76d87162",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "VECTOR STORE PREPARATION\n",
      "============================================================\n",
      "üìÅ Created: ../vector_store\n",
      "üíæ Config saved: ../vector_store\\config.json\n",
      "üìù README created: ../vector_store\\README.md\n",
      "\n",
      "============================================================\n",
      "‚úÖ TASK 2 COMPLETE!\n",
      "============================================================\n",
      "\n",
      "üìã **Deliverables Completed:**\n",
      "\n",
      "1. **‚úÖ Stratified Sample Created**\n",
      "   - 10,000 chunks with proportional product distribution\n",
      "   - Saved to: data/processed/sample_chunks.csv\n",
      "\n",
      "2. **‚úÖ Chunking Strategy Documented**\n",
      "   - 500 characters per chunk\n",
      "   - 50 character overlap\n",
      "   - Recursive splitting method\n",
      "\n",
      "3. **‚úÖ Embedding Model Selected**\n",
      "   - sentence-transformers/all-MiniLM-L6-v2\n",
      "   - 384 dimensions\n",
      "   - Optimized for semantic search\n",
      "\n",
      "4. **‚úÖ Vector Store Prepared**\n",
      "   - Directory structure created\n",
      "   - Configuration file saved\n",
      "   - Ready for Task 3\n",
      "\n",
      "5. **‚úÖ Documentation Complete**\n",
      "   - This notebook\n",
      "   - All decisions documented\n",
      "\n",
      "üìÅ **Files Created:**\n",
      "- data/processed/sample_chunks.csv\n",
      "- vector_store/config.json\n",
      "- vector_store/README.md\n",
      "\n",
      "üöÄ **Next: Task 3 - RAG Pipeline**\n",
      "Will use pre-built embeddings for semantic search.\n",
      "\n",
      "\n",
      "‚è±Ô∏è  Task 2 completed in seconds!\n"
     ]
    }
   ],
   "source": [
    "# ## Step 6: Prepare Vector Store\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"VECTOR STORE PREPARATION\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Create directory\n",
    "vector_store_dir = \"../vector_store\"\n",
    "os.makedirs(vector_store_dir, exist_ok=True)\n",
    "print(f\"üìÅ Created: {vector_store_dir}\")\n",
    "\n",
    "# Save simple config\n",
    "config = {\n",
    "    \"note\": \"Vector store for CrediTrust RAG system\",\n",
    "    \"chunk_size\": 500,\n",
    "    \"chunk_overlap\": 50,\n",
    "    \"embedding_model\": \"all-MiniLM-L6-v2\",\n",
    "    \"dimensions\": 384,\n",
    "    \"sample_created\": f\"{len(df_sample):,} chunks\",\n",
    "    \"next_step\": \"Task 3: Load pre-built embeddings into ChromaDB\"\n",
    "}\n",
    "\n",
    "config_path = os.path.join(vector_store_dir, \"config.json\")\n",
    "with open(config_path, 'w') as f:\n",
    "    json.dump(config, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Config saved: {config_path}\")\n",
    "\n",
    "# Create README\n",
    "readme_content = \"\"\"# Vector Store\n",
    "\n",
    "For Task 3, we will:\n",
    "1. Load pre-built embeddings from data/raw/complaint_embeddings.parquet\n",
    "2. Create ChromaDB collection\n",
    "3. Build semantic search retriever\n",
    "\n",
    "Note: Embeddings are pre-computed (2.2GB file).\n",
    "\"\"\"\n",
    "\n",
    "readme_path = os.path.join(vector_store_dir, \"README.md\")\n",
    "with open(readme_path, 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(f\"üìù README created: {readme_path}\")\n",
    "\n",
    "# %% [markdown]\n",
    "# ## Step 7: Task 2 Deliverables Complete\n",
    "\n",
    "# %%\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"‚úÖ TASK 2 COMPLETE!\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(\"\"\"\n",
    "üìã **Deliverables Completed:**\n",
    "\n",
    "1. **‚úÖ Stratified Sample Created**\n",
    "   - 10,000 chunks with proportional product distribution\n",
    "   - Saved to: data/processed/sample_chunks.csv\n",
    "\n",
    "2. **‚úÖ Chunking Strategy Documented**\n",
    "   - 500 characters per chunk\n",
    "   - 50 character overlap\n",
    "   - Recursive splitting method\n",
    "\n",
    "3. **‚úÖ Embedding Model Selected**\n",
    "   - sentence-transformers/all-MiniLM-L6-v2\n",
    "   - 384 dimensions\n",
    "   - Optimized for semantic search\n",
    "\n",
    "4. **‚úÖ Vector Store Prepared**\n",
    "   - Directory structure created\n",
    "   - Configuration file saved\n",
    "   - Ready for Task 3\n",
    "\n",
    "5. **‚úÖ Documentation Complete**\n",
    "   - This notebook\n",
    "   - All decisions documented\n",
    "\n",
    "üìÅ **Files Created:**\n",
    "- data/processed/sample_chunks.csv\n",
    "- vector_store/config.json\n",
    "- vector_store/README.md\n",
    "\n",
    "üöÄ **Next: Task 3 - RAG Pipeline**\n",
    "Will use pre-built embeddings for semantic search.\n",
    "\"\"\")\n",
    "\n",
    "print(f\"\\n‚è±Ô∏è  Task 2 completed in seconds!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f33cf1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
